{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook:\n",
    "- setting up variables used in multiple places in the notebook\n",
    "- text generation\n",
    "- Bias evaluation of the generated texts\n",
    "- Calculating bias scores\n",
    "- fitting the amplitude regression model ($\\beta$)\n",
    "- calculating final perplexities\n",
    "\n",
    "This all results in the data we show in the report \n",
    "\n",
    "# Preliminaries\n",
    "\n",
    "Please set DEVICE_STR appropriatly for your machine at the top of the cell directly below, if you have a CUDA-compatible GPU this greatly speeds up parts of the notebook\n",
    "\n",
    "Furthermore, before continuing downward in the notebook, make sure the trained models are downloaded from [this dropbox URL](https://www.dropbox.com/s/7hqln183f14vh3a/models.zip?dl=0) and are saved in the directory FACT-replicate/models. for example, a model for the CNN/Daily mail dataset trained without debiasing should be located as follows from the perspective of the top-level directory\n",
    "\n",
    "model/models/dataset_dm_encoding_lmbd_0.0_decoding_lmbd_0_ASGD_True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "assert os.path.exists(os.path.join(\"models\", \"dataset_dm_encoding_lmbd_0.0_decoding_lmbd_0_ASGD_True\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch device for evaluating models & generating text\n",
    "DEVICE_STR = \"cpu\" # alternatively 'cuda:0'\n",
    "DEVICE_STR = \"cuda:0\" # alternatively 'cpu'\n",
    "\n",
    "model_names = [\"dataset_penn_encoding_lmbd_0.0_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_penn_encoding_lmbd_0.001_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_penn_encoding_lmbd_0.01_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_penn_encoding_lmbd_0.1_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_penn_encoding_lmbd_0.5_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_penn_encoding_lmbd_0.8_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_penn_encoding_lmbd_1.0_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_dm_encoding_lmbd_0.0_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_dm_encoding_lmbd_0.001_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_dm_encoding_lmbd_0.01_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_dm_encoding_lmbd_0.1_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_dm_encoding_lmbd_0.5_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_dm_encoding_lmbd_0.8_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_dm_encoding_lmbd_1.0_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_wikitext-2_encoding_lmbd_0.0_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_wikitext-2_encoding_lmbd_0.001_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_wikitext-2_encoding_lmbd_0.01_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_wikitext-2_encoding_lmbd_0.1_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_wikitext-2_encoding_lmbd_0.5_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_wikitext-2_encoding_lmbd_0.8_decoding_lmbd_0_ASGD_True\",\n",
    "                \"dataset_wikitext-2_encoding_lmbd_1.0_decoding_lmbd_0_ASGD_True\"]\n",
    "\n",
    "\n",
    "dataset_names = [\"penn\"]*7 + [\"dm\"]*7 + [\"wiki\"]*7\n",
    "lambdas = [\"0.0\", \"0.001\", \"0.01\", \"0.1\", \"0.5\", \"0.8\", \"1.0\"] * 3\n",
    "dataset_names_eval = [\"penn\"]*7 + [\"dm\"]*7 + [\"wikitext-2\"]*7\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation\n",
    "\n",
    "In the following part we generate text from our trained language models\n",
    "this notebook has been adjusted to have a lower runtime by generating fewer files.\n",
    "\n",
    "Words_per_file is left at 500 as we reported, while files has been reduced from 2000 to 500, to reduce runtime for checking functionality.\n",
    "\n",
    "If you are interested to get our results, you can change the files variable to 2000. this causes more text (from more different seeds) to be generated for further evaluation\n",
    "\n",
    "Generating 2000 files takes approximately 1.5 - 2 hours **per model**, 500 files cuts down on this a bit but this does take a long time regardless. Creating very few files might save a lot of time, but also makes it possible to generate fewer gendered words (or none at all), which makes the bias Calculation less accurate (and if no gendered words are generated, this causes the model to fail)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = 500\n",
    "word_per_file = 500\n",
    "\n",
    "TEXT_FOLDER = './generated/'\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    lmbda = lambdas[i]\n",
    "    dataset = dataset_names_eval[i]\n",
    "    print(\"dataset\", dataset)\n",
    "    print(\"model_name\", model_name)\n",
    "    %run \"text_generation.py\" --dataset $dataset --model_name $model_name --device $DEVICE_STR --words $word_per_file --files $files --lmbda $lmbda --generate_folder $TEXT_FOLDER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias evaluation\n",
    "\n",
    "Now that we've generated text using the trained language models, we calculate the bias scores per model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bias_calculator import BiasCalculator\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "calc = BiasCalculator()\n",
    "calc.calculate_bias('./data', TEXT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression of the amplitude $\\beta$\n",
    "\n",
    "given that we now have calculated the bias for each of the words, we will now fit the linear regression model for $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beta_calculator import BetaCalculator\n",
    "\n",
    "\n",
    "calculator = BetaCalculator()\n",
    "calculator.calculate_beta('./output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate perplexity values\n",
    "\n",
    "To get a complete overview of the results we calculate the final perplexity values for each of the trained models. This took approximately 30 minutes on one of our GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_perplexity import calculate_perplexities\n",
    "\n",
    "model_path = os.path.join(os.path.curdir,\"models\")\n",
    "\n",
    "calculate_perplexities(model_names, dataset_names_eval, model_path, DEVICE_STR, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result overview:\n",
    "\n",
    "The next cell show a comparison between the overall results of our implementation and that of\n",
    "Bordia & Bowman. Theirs is shown on the left, ours on the right.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdict = {}\n",
    "\n",
    "datasets = set()\n",
    "\n",
    "version = \"reproduced\"\n",
    "\n",
    "\n",
    "with open(os.path.join(\"results\",\"beta-scores-original-gender-pairs\"), \"r\") as f:\n",
    "    beta_data = f.readlines()\n",
    "    for i, line in enumerate(beta_data):\n",
    "        if not (i % 2):\n",
    "            dataset, lambda_, context = line[:-1].split()\n",
    "            datasets.add(dataset)\n",
    "        else:\n",
    "            beta, c = line[:-1].split(\",\")\n",
    "            beta = beta.split(\":\")[1].strip()\n",
    "            c = c.split(\":\")[1].strip()\n",
    "            resdict[(version, dataset, lambda_, context)] = {\"beta\":beta, \"c\":c}\n",
    "\n",
    "\n",
    "with open(os.path.join(\"results\",\"bias-scores-original-gender-pairs\"), \"r\") as f:\n",
    "    beta_data = f.readlines()\n",
    "    for i, line in enumerate(beta_data):\n",
    "        if not (i % 2):\n",
    "            dataset, lambda_ = line[:-1].split()\n",
    "        else:\n",
    "            duo_context_dict = eval(line[:-1])\n",
    "            \n",
    "            if resdict.get((version, dataset, lambda_, context), False):\n",
    "                resdict[(version, dataset, lambda_, \"fixed\")].update(duo_context_dict[\"fixed\"])\n",
    "                resdict[(version, dataset, lambda_, \"infinite\")].update(duo_context_dict[\"infinite\"])\n",
    "            else:\n",
    "\n",
    "                resdict[(version, dataset, lambda_, \"fixed\")] = duo_context_dict[\"fixed\"]\n",
    "                resdict[(version, dataset, lambda_, \"infinite\")] = duo_context_dict[\"infinite\"]\n",
    "\n",
    "\n",
    "                \n",
    "version = \"original\"                \n",
    "with open(os.path.join(os.path.join(os.path.curdir,\"results\"),\"OP_results_reference.txt\"), \"r\") as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if i < 2:\n",
    "            pass\n",
    "#             print(line[:-1])\n",
    "        else:\n",
    "            if len(line[:-1].split()) < 2:\n",
    "                dataset = line[:-1]\n",
    "            else:\n",
    "                line_data = line[:-1].split()\n",
    "                lambda_, mu_fixed, sigma_fixed, beta_fixed, mu_infinite, sigma_infinite, beta_infinite, ppl = line_data\n",
    "                \n",
    "                resdict[(version, dataset, lambda_, \"fixed\")] = {\"std abs\":sigma_fixed, \"mean abs\":mu_fixed, \"beta\":beta_fixed}\n",
    "                resdict[(version, dataset, lambda_, \"infinite\")] = {\"std abs\":sigma_infinite, \"mean abs\":mu_infinite, \"beta\":beta_infinite}\n",
    "                resdict[(version, dataset, lambda_)] = {\"ppl\":ppl}\n",
    "    \n",
    "\n",
    "version = \"reproduced\"\n",
    "with open(\"perplexity_result_file.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line_data = line[:-1].split()\n",
    "        dataset, lambda_, ppl = line_data\n",
    "        resdict[(version, dataset, lambda_)] = {\"ppl\":ppl}\n",
    "        \n",
    "        \n",
    "\n",
    "version = \"reproduced\"\n",
    "\n",
    "context_types = [\"fixed\", \"infinite\"]\n",
    "ordered_table_keys = ['mean abs', 'std abs', 'beta']\n",
    "lambdas = ['train','0.0', '0.001', '0.01', '0.1', '0.5', '0.8', '1.0']\n",
    "\n",
    "overview_rows = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    overview_rows.append([dataset, \"\", \"Original\", \"\", \"\", \"\", \"ours\", \"\"])\n",
    "    for context_type in context_types:\n",
    "        overview_rows.append([context_type + \" context\"])\n",
    "        overview_rows.append([\"$\\lambda$\",\"$\\sigma$\", \"$\\mu$\", \"$\\\\beta$\", \"$\\\\textit{ppl}$\"] + [\"$\\sigma$\", \"$\\mu$\", \"$\\\\beta$\", \"$\\\\textit{ppl}$\"])\n",
    "        for lambda_ in lambdas:\n",
    "            rd_repr = resdict.get((\"reproduced\", dataset, lambda_, context_type), '-')\n",
    "            rd_orig = resdict.get((\"original\", dataset, lambda_, context_type), '-')\n",
    "            if rd_orig == '-':\n",
    "                row_orig = ['-']*3\n",
    "                row_repr = [str(rd_repr.get(k, '-'))[:4] for k in ordered_table_keys]\n",
    "            else:\n",
    "                row_orig = [str(rd_orig.get(k, '-'))[:4] for k in ordered_table_keys]\n",
    "                row_repr = [str(rd_repr.get(k, '-'))[:4] for k in ordered_table_keys]\n",
    "\n",
    "            ppl_orig = resdict.get((\"original\", dataset, lambda_), '-')\n",
    "            if isinstance(ppl_orig, dict):\n",
    "                ppl_orig = ppl_orig.get('ppl')\n",
    "\n",
    "                \n",
    "            ppl_repr = resdict.get((\"reproduced\", dataset, lambda_), '-')\n",
    "            if isinstance(ppl_repr, dict):\n",
    "                ppl_repr = ppl_repr.get('ppl')\n",
    "\n",
    "            \n",
    "            overview_row = [lambda_] + row_orig + [ppl_orig] + row_repr + [ppl_repr]\n",
    "            overview_rows.append(overview_row)\n",
    "    overview_rows.append([\"\"])\n",
    "\n",
    "# used to generate part of the latex\n",
    "\n",
    "# for overview_row in overview_rows: \n",
    "#     if isinstance(overview_row, list):\n",
    "#         print(\" & \".join(overview_row) + \"\\\\\\\\\")\n",
    "#     else:\n",
    "#         print(overview_row)\n",
    "\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "display(HTML(tabulate.tabulate(overview_rows, tablefmt='html')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
